{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76263787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter product name you want to search: guitar\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 1. Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search \n",
    "for guitars.\"\"\"\n",
    "\n",
    "query = input('Enter product name you want to search: ')\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_1 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_1.get('https://www.amazon.in/')\n",
    "\n",
    "browser_1.maximize_window()\n",
    "\n",
    "click_searchbar1 = browser_1.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "click_searchbar1.click()\n",
    "click_searchbar1.send_keys(query)\n",
    "\n",
    "find_searchbutton1 = browser_1.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "find_searchbutton1.click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fda209ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2. In the above question, now scrape the following details of each product listed in first 3 pages of your \n",
    "search results and save it in a data frame and csv. In case if any product has less than 3 pages in search \n",
    "results then scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e78fa32",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1321082475.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [52]\u001b[1;36m\u001b[0m\n\u001b[1;33m    'Mozilla/5.0 (X11; Linux x86_64)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def main(URL):\n",
    "    # opening our output file in append mode\n",
    "    File = open(\"out.csv\", \"a\")\n",
    "\n",
    "    # specifying user agent, You can use other user agents\n",
    "    # available on the internet\n",
    "    HEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64)\n",
    "                    AppleWebKit/537.36 (KHTML, like Gecko)\n",
    "                        Chrome/44.0.2403.157 Safari/537.36',\n",
    "                            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # Making the HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Creating the Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "    # retrieving product title\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\",\n",
    "                        attrs={\"id\": 'productTitle'})\n",
    "\n",
    "        # Inner NavigableString Object\n",
    "        title_value = title.string\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"NA\"\n",
    "    print(\"product Title = \", title_string)\n",
    "\n",
    "    # saving the title in the file\n",
    "    File.write(f\"{title_string},\")\n",
    "\n",
    "    # retrieving price\n",
    "    try:\n",
    "        price = soup.find(\n",
    "            \"span\", attrs={'id': 'priceblock_ourprice'})\n",
    "                                .string.strip().replace(',', '')\n",
    "        # we are omitting unnecessary spaces\n",
    "        # and commas form our string\n",
    "    except AttributeError:\n",
    "        price = \"NA\"\n",
    "    print(\"Products price = \", price)\n",
    "\n",
    "    # saving\n",
    "    File.write(f\"{price},\")\n",
    "\n",
    "    # retrieving product rating\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={\n",
    "                        'class': 'a-icon a-icon-star a-star-4-5'})\n",
    "                                    .string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            rating = soup.find(\n",
    "                \"span\", attrs={'class': 'a-icon-alt'})\n",
    "                                .string.strip().replace(',', '')\n",
    "        except:\n",
    "            rating = \"NA\"\n",
    "    print(\"Overall rating = \", rating)\n",
    "\n",
    "    File.write(f\"{rating},\")\n",
    "\n",
    "    try:\n",
    "        review_count = soup.find(\n",
    "            \"span\", attrs={'id': 'acrCustomerReviewText'})\n",
    "                                .string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"NA\"\n",
    "    print(\"Total reviews = \", review_count)\n",
    "    File.write(f\"{review_count},\")\n",
    "\n",
    "    # print availablility status\n",
    "    try:\n",
    "        available = soup.find(\"div\", attrs={'id': 'availability'})\n",
    "        available = available.find(\"span\")\n",
    "                .string.strip().replace(',', '')\n",
    "\n",
    "    except AttributeError:\n",
    "        available = \"NA\"\n",
    "    print(\"Availability = \", available)\n",
    "\n",
    "    # saving the availability and closing the line\n",
    "    File.write(f\"{available},\\n\")\n",
    "\n",
    "    # closing the file\n",
    "    File.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# opening our url file to access URLs\n",
    "    file = open(\"url.txt\", \"r\")\n",
    "\n",
    "    # iterating over the urls\n",
    "    for links in file.readlines():\n",
    "        main(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc964c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sponsored',\n",
       " 'Kadence Slowhand Premium Jumbo Semi Acoustic Guitar with Heavy Padded Bag, guitar cable, Pro Capo (Black Spruce Wood)',\n",
       " '534',\n",
       " 'Limited time deal',\n",
       " '₹9,999',\n",
       " '₹16,779 (40% off)',\n",
       " 'Get it by Monday, November 28',\n",
       " 'FREE Delivery by Amazon']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alld[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c7b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\"\"\"\n",
    "\n",
    "#fruitsimg \n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_3 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_3.get('https://www.google.com/')\n",
    "\n",
    "browser_3.maximize_window()\n",
    "\n",
    "enter_name3 = browser_3.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "enter_name3.send_keys('fruits')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "slsearch3 = browser_3.find_element(By.XPATH,\"//input[@class='gNO89b']\")\n",
    "slsearch3.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imgbtn3 = browser_3.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "imgbtn3.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "images = []\n",
    "ele = browser_3.find_elements(By.XPATH,\"//img[@src]\")\n",
    "for e in ele:\n",
    "    images.append(e.get_attribute('src'))\n",
    "\n",
    "fltrimg = list(filter(lambda k: 'data' in k , images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fltrimg[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaef916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#carsimg \n",
    "\n",
    "browser_3cars = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_3cars.get('https://www.google.com/')\n",
    "browser_3cars.maximize_window()\n",
    "enter_name3 = browser_3cars.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "enter_name3.send_keys('cars')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "slsearch3 = browser_3cars.find_element(By.XPATH,\"//input[@class='gNO89b']\")\n",
    "slsearch3.click()\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imgbtn3cars = browser_3cars.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "imgbtn3cars.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imagescars = []\n",
    "ele = browser_3cars.find_elements(By.XPATH,\"//img[@src]\")\n",
    "for e in ele:\n",
    "    imagescars.append(e.get_attribute('src'))\n",
    "\n",
    "fltrimgcars = list(filter(lambda k: 'data' in k , imagescars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#machinelearning \n",
    "\n",
    "browser_3ml= webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_3ml.get('https://www.google.com/')\n",
    "browser_3ml.maximize_window()\n",
    "enter_name3ml = browser_3ml.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "enter_name3ml.send_keys('machine learning')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "slsearch3ml = browser_3ml.find_element(By.XPATH,\"//input[@class='gNO89b']\")\n",
    "slsearch3ml.click()\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imgbtn3ml = browser_3ml.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "imgbtn3ml.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imagesml = []\n",
    "ele = browser_3ml.find_elements(By.XPATH,\"//img[@src]\")\n",
    "for e in ele:\n",
    "    imagesml.append(e.get_attribute('src'))\n",
    "\n",
    "fltrimgml = list(filter(lambda k: 'data' in k , imagesml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b43f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guitar\n",
    "\n",
    "browser_3g = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_3g.get('https://www.google.com/')\n",
    "browser_3g.maximize_window()\n",
    "enter_name3g = browser_3g.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "enter_name3g.send_keys('guitar')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "slsearch3g = browser_3g.find_element(By.XPATH,\"//input[@class='gNO89b']\")\n",
    "slsearch3g.click()\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imgbtn3g = browser_3g.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "imgbtn3g.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imagesg = []\n",
    "ele = browser_3g.find_elements(By.XPATH,\"//img[@src]\")\n",
    "for e in ele:\n",
    "    imagesg.append(e.get_attribute('src'))\n",
    "\n",
    "fltrimgg = list(filter(lambda k: 'data' in k , imagesg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cakes\n",
    "\n",
    "browser_3ck = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_3ck.get('https://www.google.com/')\n",
    "browser_3ck.maximize_window()\n",
    "enter_name3ck = browser_3ck.find_element(By.XPATH,\"//input[@class='gLFyf']\")\n",
    "enter_name3ck.send_keys('cakes')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "slsearch3ck = browser_3ck.find_element(By.XPATH,\"//input[@class='gNO89b']\")\n",
    "slsearch3ck.click()\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imgbtn3ck = browser_3ck.find_element(By.XPATH,\"/html/body/div[7]/div/div[4]/div/div[1]/div/div[1]/div/div[2]/a\")\n",
    "imgbtn3ck.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "imagesck = []\n",
    "ele = browser_3ck.find_elements(By.XPATH,\"//img[@src]\")\n",
    "for e in ele:\n",
    "    imagesck.append(e.get_attribute('src'))\n",
    "\n",
    "fltrimgck = list(filter(lambda k: 'data' in k , imagesck))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img['Fruits images']=[fltrimg[0:10]]\n",
    "df_img['Cars img']=[fltrimgcars[0:10]]\n",
    "df_img['ML img']=[fltrimgml[0:10]]\n",
    "df_img['Guitar img']=[fltrimgg[0:10]]\n",
    "df_img['Cake images']=[fltrimgck[0:10]]\n",
    "\n",
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be \n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a soup object using driver.page_source to retreive the HTML text and then we'll use the default html parser to parse\n",
    "# the HTML.\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "def extract_phone_model_info(item):\n",
    "    \"\"\"\n",
    "    This function extracts model, price, ram, storage, stars , number of ratings, number of reviews, \n",
    "    storage expandable option, display option, camera quality, battery , processor, warranty of a phone model at flipkart\n",
    "    \"\"\"\n",
    "    # Extracting the model of the phone from the 1st card\n",
    "    model = item.find('div',{'class':\"_4rR01T\"}).text\n",
    "    # Extracting Stars from 1st card\n",
    "    star = item.find('div',{'class':\"_3LWZlK\"}).text\n",
    "    # Extracting Number of Ratings from 1st card\n",
    "    num_ratings = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[0:item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')].strip()\n",
    "    # Extracting Number of Reviews from 1st card\n",
    "    reviews = item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \")[item.find('span',{'class':\"_2_R_DZ\"}).text.replace('\\xa0&\\xa0',\" ; \").find(';')+1:].strip()\n",
    "    # Extracting RAM from the 1st card\n",
    "    ram = item.find('li',{'class':\"rgWa7D\"}).text[0:item.find('li',{'class':\"rgWa7D\"}).text.find('|')]\n",
    "    # Extracting Storage/ROM from 1st card\n",
    "    storage = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][0:10].strip()\n",
    "    # Extracting whether there is an option of expanding the storage or not\n",
    "    expandable = item.find('li',{'class':\"rgWa7D\"}).text[item.find('li',{'class':\"rgWa7D\"}).text.find('|')+1:][13:]\n",
    "    # Extracting the display option from the 1st card\n",
    "    display = item.find_all('li')[1].text.strip()\n",
    "    # Extracting camera options from the 1st card\n",
    "    camera = item.find_all('li')[2].text.strip()\n",
    "    # Extracting the battery option from the 1st card\n",
    "    battery = item.find_all('li')[3].text\n",
    "    # Extracting the processir option from the 1st card\n",
    "    processor = item.find_all('li')[4].text.strip()\n",
    "    # Extracting Warranty from the 1st card\n",
    "    warranty = item.find_all('li')[-1].text.strip()\n",
    "    # Extracting price of the model from the 1st card\n",
    "    price = item.find('div',{'class':'_30jeq3 _1_WHN1'}).text\n",
    "    result = (model,star,num_ratings,reviews,ram,storage,expandable,display,camera,battery,processor,warranty,price)\n",
    "    return result\n",
    "\n",
    "\n",
    "def main(search_item):\n",
    "    '''\n",
    "    This function will create a dataframe for all the details that we are fetching from all the multiple pages\n",
    "    '''\n",
    "    driver = webdriver.Chrome(executable_path=path)\n",
    "    records = []\n",
    "    url = get_url(search_item)\n",
    "    for page in range(1):\n",
    "        driver.get(url.format(page))\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        results = soup.find_all('a',{'class':\"_1fQZEK\"})\n",
    "        for item in results:\n",
    "            records.append(extract_phone_model_info(item))\n",
    "    driver.close()\n",
    "    # Saving the data into a csv file\n",
    "    with open('Flipkart_results.csv','w',newline='',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Model','Stars','Num_of_Ratings','Reviews','Ram','Storage','Expandable',\n",
    "                        'Display','Camera','Battery','Processor','Warranty','Price'])\n",
    "        writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('mobile phones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "maps.\n",
    "\"\"\"\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "browser_5 = webdriver.Chrome(executable_path=path)\n",
    "wait = WebDriverWait(browser_5, 10)\n",
    "browser_5.get(\"https://www.google.com/maps\")\n",
    "browser_5.maximize_window()\n",
    "wait.until(EC.element_to_be_clickable((By.ID, \"searchboxinput\"))).send_keys(\"New York\")\n",
    "wait.until(EC.element_to_be_clickable((By.ID, \"searchbox-searchbutton\"))).click()\n",
    "sleep(5)\n",
    "ActionChains(browser_5).move_to_element(browser_5.find_element(By.XPATH,\"//html/body\")).context_click().perform()\n",
    "print(wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \"ul[role='menu']>li div div[class*='text']:nth-of-type(1)\"))).text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5180ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21) \n",
    "from trak.in\"\"\"\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_6 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_6.get('https://www.trak.in/')\n",
    "\n",
    "browser_6.maximize_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606087fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"7. Write a program to scrap all the available details of best gaming laptops from digit.in.\"\"\"\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_2 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_2.get('https://www.digit.in/')\n",
    "\n",
    "browser_2.maximize_window()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "best_laptops_button = browser_2.find_element(By.XPATH,'/html/body/div[3]/div/div[2]/div[2]/div[4]/ul/li[9]/a')\n",
    "best_laptops_button.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "lptops = []\n",
    "lptitle = browser_2.find_elements(By.XPATH,\"//div[@class='left_side']\")\n",
    "for i in lptitle:\n",
    "    lptops.append(i.text.replace('\\n',''))\n",
    "    \n",
    "lptops.pop(10)\n",
    "time.sleep(3)\n",
    "\n",
    "specsdet = []\n",
    "specs = browser_2.find_elements(By.XPATH,\"//div[@class='Spcs-details']\")\n",
    "for j in specs:\n",
    "    specsdet.append(j.text.replace('MORE SPECIFICATIONS\\n',''))\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "desc = []\n",
    "des = browser_2.find_elements(By.XPATH,\"//div[@class='tptn-prod-desc']\")\n",
    "for k in des:\n",
    "    desc.append(k.text)\n",
    "    \n",
    "time.sleep(3)\n",
    "\n",
    "df_lptps = pd.DataFrame()\n",
    "df_lptps['Laptop Name'] = lptops\n",
    "df_lptps['Specification'] = specsdet\n",
    "df_lptps['Description'] = desc\n",
    "\n",
    "df_lptps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33370574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be \n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”\"\"\"\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_8 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_8.get('https://www.forbes.com/')\n",
    "\n",
    "browser_8.maximize_window()\n",
    "\n",
    "side_button8 = browser_8.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[1]')\n",
    "side_button8.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "billionairebutton8 = browser_8.find_element(By.XPATH,'/html/body/div[1]/header/nav/div[1]/div/div/div[2]/ul/li[1]')\n",
    "billionairebutton8.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "wbnrsbutton8 = browser_8.find_element(By.XPATH,'/html/body/div[1]/main/div/section/section[1]/div/div/div[1]/div/div[1]/div[1]/div[2]/a/h2')\n",
    "wbnrsbutton8.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "rank = []\n",
    "names = []\n",
    "netwrth = []\n",
    "cship = []\n",
    "age = []\n",
    "category = []\n",
    "source = []\n",
    "\n",
    "for page in range(1,15):\n",
    "    time.sleep(3)\n",
    "    rk = browser_8.find_elements(By.XPATH,\"//div[@class='rank']\")\n",
    "    for r in rk:\n",
    "        rank.append(r.text) \n",
    "    time.sleep(3)\n",
    "    nlist = browser_8.find_elements(By.XPATH,\"//div[@class='personName']\")\n",
    "    for i in nlist:\n",
    "        names.append(i.text)     \n",
    "    time.sleep(3)\n",
    "    wrth = browser_8.find_elements(By.XPATH,\"//div[@class='netWorth']\")\n",
    "    for j in wrth:\n",
    "        netwrth.append(j.text)\n",
    "    time.sleep(3)\n",
    "    ctzn = browser_8.find_elements(By.XPATH,\"//div[@class='countryOfCitizenship']\")\n",
    "    for k in ctzn:\n",
    "        cship.append(k.text)\n",
    "    time.sleep(3)\n",
    "    ag = browser_8.find_elements(By.XPATH,\"//div[@class='age']\")\n",
    "    for l in ag:\n",
    "        age.append(l.text)\n",
    "    time.sleep(3)\n",
    "    cat = browser_8.find_elements(By.XPATH,\"//div[@class='category']\")\n",
    "    for m in cat:\n",
    "        category.append(m.text)\n",
    "    time.sleep(3)\n",
    "    sr = browser_8.find_elements(By.XPATH,\"//div[@class='source']\")\n",
    "    for s in sr:\n",
    "        source.append(s.text) \n",
    "    time.sleep(3)\n",
    "    next_button8 = browser_8.find_element(By.CLASS_NAME,\"goto-page__btn\")\n",
    "    next_button8.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f74efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Names</th>\n",
       "      <th>Age</th>\n",
       "      <th>Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Category</th>\n",
       "      <th>Networth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>50</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>$219 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>58</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "      <td>$171 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>73</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "      <td>$158 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>66</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "      <td>$129 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Warren Buffett</td>\n",
       "      <td>91</td>\n",
       "      <td>United States</td>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "      <td>$118 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>192.</td>\n",
       "      <td>Marcel Herrmann Telles</td>\n",
       "      <td>72</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>beer</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "      <td>$10.3 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197.</td>\n",
       "      <td>Leon Black</td>\n",
       "      <td>70</td>\n",
       "      <td>United States</td>\n",
       "      <td>private equity</td>\n",
       "      <td>Finance &amp; Investments</td>\n",
       "      <td>$10 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197.</td>\n",
       "      <td>Joe Gebbia</td>\n",
       "      <td>40</td>\n",
       "      <td>United States</td>\n",
       "      <td>Airbnb</td>\n",
       "      <td>Technology</td>\n",
       "      <td>$10 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>197.</td>\n",
       "      <td>David Geffen</td>\n",
       "      <td>79</td>\n",
       "      <td>United States</td>\n",
       "      <td>movies, record labels</td>\n",
       "      <td>Media &amp; Entertainment</td>\n",
       "      <td>$10 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>197.</td>\n",
       "      <td>Yu Renrong</td>\n",
       "      <td>56</td>\n",
       "      <td>China</td>\n",
       "      <td>semiconductors</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>$10 B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rank                     Names Age    Citizenship                 Source  \\\n",
       "0      1.                 Elon Musk  50  United States          Tesla, SpaceX   \n",
       "1      2.                Jeff Bezos  58  United States                 Amazon   \n",
       "2      3.  Bernard Arnault & family  73         France                   LVMH   \n",
       "3      4.                Bill Gates  66  United States              Microsoft   \n",
       "4      5.            Warren Buffett  91  United States     Berkshire Hathaway   \n",
       "..    ...                       ...  ..            ...                    ...   \n",
       "195  192.    Marcel Herrmann Telles  72         Brazil                   beer   \n",
       "196  197.                Leon Black  70  United States         private equity   \n",
       "197  197.                Joe Gebbia  40  United States                 Airbnb   \n",
       "198  197.              David Geffen  79  United States  movies, record labels   \n",
       "199  197.                Yu Renrong  56          China         semiconductors   \n",
       "\n",
       "                  Category Networth  \n",
       "0               Automotive   $219 B  \n",
       "1               Technology   $171 B  \n",
       "2         Fashion & Retail   $158 B  \n",
       "3               Technology   $129 B  \n",
       "4    Finance & Investments   $118 B  \n",
       "..                     ...      ...  \n",
       "195        Food & Beverage  $10.3 B  \n",
       "196  Finance & Investments    $10 B  \n",
       "197             Technology    $10 B  \n",
       "198  Media & Entertainment    $10 B  \n",
       "199          Manufacturing    $10 B  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bnrs = pd.DataFrame()\n",
    "df_bnrs['Rank'] = rank\n",
    "df_bnrs['Names'] = names\n",
    "df_bnrs['Age'] = age\n",
    "df_bnrs['Citizenship'] = cship\n",
    "df_bnrs['Source'] = source\n",
    "df_bnrs['Category'] = category\n",
    "df_bnrs['Networth'] = netwrth\n",
    "\n",
    "df_bnrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video.\"\"\"\n",
    "\n",
    "import time\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# open chrome \n",
    "youtube_pages = \"https://www.youtube.com/\"\n",
    "path = 'C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=path)\n",
    "driver.get(youtube_pages)\n",
    "time.sleep(10)\n",
    "try:\n",
    "    print(\"=\" * 40)  # Shows in terminal when youtube summary page with search keyword is being scraped\n",
    "    print(\"Scraping \" + youtube_pages)\n",
    "    search = driver.find_element(By.CLASS_NAME,'ytd-searchbox')\n",
    "    search.send_keys(\"Kishore Kumar\")    \n",
    "    driver.find_element(By.CLASS_NAME,'style-scope ytd-searchbox').click()\n",
    "    time.sleep(20)    \n",
    "    vtitle = driver.find_elements(By.ID,'video-title')\n",
    "    subscription = driver.find_elements(By.ID,'byline')\n",
    "    views = driver.find_elements(By.XPATH,'//div[@id=\"metadata-line\"]/span[1]')\n",
    "    posted = driver.find_elements(By.XPATH,'//div[@id=\"metadata-line\"]/span[2]')\n",
    "    \n",
    "    tcount = 0\n",
    "    href = []\n",
    "    title = []\n",
    "    channel = []\n",
    "    numview = []\n",
    "    postdate = []\n",
    "        \n",
    "    while tcount < 10:\n",
    "        href.append(vtitle[tcount].get_attribute('href'))\n",
    "        channel.append(subscription[tcount].get_attribute('title'))\n",
    "        title.append(vtitle[tcount].text)\n",
    "        numview.append(views[tcount].text)\n",
    "        postdate.append(posted[tcount].text)  \n",
    "        tcount = tcount +1\n",
    "    \n",
    "    # launch top ten extracted links and extract comment details\n",
    "    tcount = 0    \n",
    "    while tcount < 10:  \n",
    "        youtube_dict ={}\n",
    "        # extract comment section of top ten links\n",
    "        url = href[tcount]\n",
    "        print (url)\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            print(\"+\" * 40)  # Shows in terminal when details of a new video is being scraped\n",
    "            print(\"Scraping child links \")\n",
    "            #scroll down to load comments\n",
    "            driver.execute_script('window.scrollTo(0,390);')\n",
    "            time.sleep(15)\n",
    "            #sort by top comments\n",
    "            sort= driver.find_element_by_xpath(\"\"\"//*[@id=\"icon-label\"]\"\"\")\n",
    "            sort.click()\n",
    "            time.sleep(10)\n",
    "            topcomments =driver.find_element_by_xpath(\"\"\"//*[@id=\"menu\"]/a[1]/paper-item/paper-item-body/div[1]\"\"\")\n",
    "            topcomments.click()\n",
    "            time.sleep(10)\n",
    "            # Loads 20 comments , scroll two times to load next set of 40 comments. \n",
    "            for i in range(0,2):\n",
    "                driver.execute_script(\"window.scrollTo(0,Math.max(document.documentElement.scrollHeight,document.body.scrollHeight,document.documentElement.clientHeight))\")\n",
    "                time.sleep(10)\n",
    "            \n",
    "            #count total number of comments and set index to number of comments if less than 50 otherwise set as 50. \n",
    "            totalcomments= len(driver.find_elements_by_xpath(\"\"\"//*[@id=\"content-text\"]\"\"\"))\n",
    "         \n",
    "            if totalcomments < 50:\n",
    "                index= totalcomments\n",
    "            else:\n",
    "                index= 50 \n",
    "                \n",
    "            ccount = 0\n",
    "            while ccount < index: \n",
    "                try:\n",
    "                    comment = driver.find_elements_by_xpath('//*[@id=\"content-text\"]')[ccount].text\n",
    "                except:\n",
    "                    comment = \"\"\n",
    "                try:\n",
    "                    authors = driver.find_elements_by_xpath('//a[@id=\"author-text\"]/span')[ccount].text\n",
    "                except:\n",
    "                    authors = \"\"\n",
    "                try:\n",
    "                    comment_posted = driver.find_elements_by_xpath('//*[@id=\"published-time-text\"]/a')[ccount].text\n",
    "                except:\n",
    "                    comment_posted = \"\"\n",
    "                try:\n",
    "                    replies = driver.find_elements_by_xpath('//*[@id=\"more-text\"]')[ccount].text                    \n",
    "                    if replies ==\"View reply\":\n",
    "                        replies= 1\n",
    "                    else:\n",
    "                        replies =replies.replace(\"View \",\"\")\n",
    "                        replies =replies.replace(\" replies\",\"\")\n",
    "                except:\n",
    "                    replies = \"\"\n",
    "                try:\n",
    "                    upvotes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')[ccount].text\n",
    "                except:\n",
    "                    upvotes = \"\"\n",
    "                        \n",
    "                youtube_dict['url'] = href[tcount]\n",
    "                youtube_dict['link_title'] = title[tcount]\n",
    "                youtube_dict['channel'] = channel[tcount]\n",
    "                youtube_dict['no_of_views'] = numview[tcount]\n",
    "                youtube_dict['time_uploaded'] =  postdate[tcount]\n",
    "                youtube_dict['comment'] = comment\n",
    "                youtube_dict['author'] = authors\n",
    "                youtube_dict['comment_posted'] = comment_posted\n",
    "                youtube_dict['no_of_replies'] = replies\n",
    "                youtube_dict['upvotes'] = upvotes\n",
    "                \n",
    "                writer.writerow(youtube_dict.values())\n",
    "                ccount = ccount +1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            driver.close()\n",
    "        tcount = tcount +1 \n",
    "    print(\"Scrapping process Completed\")   \n",
    "    csv_file.close()    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    #driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dd3cd8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (16) does not match length of index (17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m df_hstls \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m     61\u001b[0m df_hstls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdet\n\u001b[1;32m---> 62\u001b[0m df_hstls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRatings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mratings\n\u001b[0;32m     63\u001b[0m df_hstls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mprp\n\u001b[0;32m     64\u001b[0m df_hstls[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFCL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mfcl10\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4535\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4535\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\common.py:557\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (16) does not match length of index (17)"
     ]
    }
   ],
   "source": [
    "\"\"\"10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, \n",
    "overall reviews, privates from price, dorms from price, facilities and property description\"\"\"\n",
    "\n",
    "path ='C:\\Program Files\\Chromedriver\\chromedriver.exe'\n",
    "\n",
    "browser_10 = webdriver.Chrome(executable_path=path)\n",
    "\n",
    "browser_10.get('https://www.hostelworld.com/')\n",
    "\n",
    "browser_10.maximize_window()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "clk_location10 = browser_10.find_element(By.XPATH,\"//input[@class='location-text']\")\n",
    "clk_location10.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "enter_location10 = browser_10.find_element(By.XPATH,\"//input[@class='search-input']\")\n",
    "enter_location10.send_keys('London')\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "click_london10= browser_10.find_element(By.XPATH,\"//div[@class='label']\")\n",
    "click_london10.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "click_sbutton10= browser_10.find_element(By.XPATH,\"//button[@class='button primary large']\")\n",
    "click_sbutton10.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "det = []\n",
    "ratings = []\n",
    "prp = []\n",
    "fcl10 = []\n",
    "des10 = []\n",
    "\n",
    "for pg in range(1):\n",
    "    time.sleep(5)\n",
    "    scrpdet10 = browser_10.find_elements(By.XPATH,\"//div[@class='title-row']\")\n",
    "    for s in scrpdet10:\n",
    "            det.append(s.text.split('\\n'))\n",
    "    scrrtn10 = browser_10.find_elements(By.XPATH,\"//div[@class='bottom-rating']\")\n",
    "    for r in scrrtn10:\n",
    "            ratings.append(r.text.split('\\n')) \n",
    "    prp10 = browser_10.find_elements(By.XPATH,\"//div[@class='price title-5']\")\n",
    "    for p in prp10:\n",
    "            prp.append(p.text.split('\\n')) \n",
    "    fcl = browser_10.find_elements(By.XPATH,\"//div[@class='facilities-label facilities']\")\n",
    "    for f in fcl:\n",
    "            fcl10.append(f.text.split('\\n')) \n",
    "    ds = browser_10.find_elements(By.XPATH,\"//div[@class='rating-factors prop-card-tablet rating-factors small']\")\n",
    "    for d in ds:\n",
    "            des10.append(d.text.split('\\n')) \n",
    "    time.sleep(3)\n",
    "    \n",
    "df_hstls = pd.DataFrame()\n",
    "df_hstls['Description']=det\n",
    "df_hstls['Ratings']=ratings\n",
    "df_hstls['Price']=prp\n",
    "df_hstls['FCL']=fcl10\n",
    "df_hstls['DES']=des10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6abcde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hstls = pd.DataFrame()\n",
    "df_hstls['Description']=det\n",
    "df_hstls['Ratings']=ratings\n",
    "df_hstls['Price']=prp\n",
    "df_hstls['FCL']=fcl10\n",
    "df_hstls['DES']=des10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bf2a0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['8.1', 'Fabulous', '11693 Total Reviews'],\n",
       " ['7.7', 'Very Good', '7257 Total Reviews'],\n",
       " ['7.1', 'Very Good', '4680 Total Reviews'],\n",
       " ['6.8', 'Good', '1376 Total Reviews'],\n",
       " ['8.3', 'Fabulous', '3899 Total Reviews'],\n",
       " ['7.1', 'Very Good', '3862 Total Reviews'],\n",
       " ['5.4', 'Rating', '3909 Total Reviews'],\n",
       " ['7.8', 'Very Good', '4156 Total Reviews'],\n",
       " ['7.3', 'Very Good', '1846 Total Reviews'],\n",
       " ['6.7', 'Good', '3278 Total Reviews'],\n",
       " ['7.4', 'Very Good', '730 Total Reviews'],\n",
       " ['8.3', 'Fabulous', '41 Total Reviews'],\n",
       " ['7.5', 'Very Good', '878 Total Reviews'],\n",
       " ['9.4', 'Superb', '1382 Total Reviews'],\n",
       " ['7.1', 'Very Good', '55 Total Reviews'],\n",
       " ['6.3', 'Good', '9 Total Reviews']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ee677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857434c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3abc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
